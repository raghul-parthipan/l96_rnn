{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data for other F regimes from L96 two-level system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "from L96_updated import *\n",
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_no_saving_of_xarray_burn_in(initial_X,initial_Y,mtu, data_file_path_to_save, data_with_y_file_path_to_save,forcing):\n",
    "    \n",
    "    \"\"\"\n",
    "    Burns in\n",
    "    \n",
    "    saves datasets with x and advection, and a separate one with x, advection and y.\n",
    "    \n",
    "    Save paths look like this: \"data/truth_run/training_dataset.npy\" and \"data/truth_run/training_dataset_with_y.npy\"\n",
    "    \n",
    "    Returns the last x and y values to init the next simulator. \n",
    "    \"\"\"\n",
    "    \n",
    "    l96_two= L96TwoLevel_updated(save_dt=save_time_step, X_init=initial_X, Y_init=initial_Y,K=k, J=J, h=1, F=forcing, c=10, b=10, dt=time_step)\n",
    "    l96_two.iterate(int(burn_in_mtu+mtu))\n",
    "    h2_xarray = l96_two.history\n",
    "    \n",
    "    x = np.ravel(h2_xarray.X)\n",
    "    x_subset = x.reshape(-1,k) #shape (timesteps, k)\n",
    "    advection = np.roll(x_subset, 1,axis=1) * (np.roll(x_subset, 2,axis=1) - np.roll(x_subset, -1,axis=1))\n",
    "    data = np.stack([x_subset,advection],axis=2)\n",
    "    y  = np.ravel(h2_xarray.Y).reshape(-1,k,J)\n",
    "    data_with_y = np.concatenate([data,y],axis=2)\n",
    "\n",
    "    del h2_xarray\n",
    "    \n",
    "    np.save(data_file_path_to_save,data[int(burn_in_mtu/save_time_step):])\n",
    "    np.save(data_with_y_file_path_to_save,data_with_y[int(burn_in_mtu/save_time_step):])\n",
    "\n",
    "    \n",
    "    initX_new = x_subset[-1,:]\n",
    "    initY_new = y[-1,:,:].reshape(k*J)\n",
    "    \n",
    "    return initX_new, initY_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_no_saving_of_xarray(initial_X,initial_Y,mtu, data_file_path_to_save, data_with_y_file_path_to_save,forcing):\n",
    "    \n",
    "    \"\"\"\n",
    "    No burn in\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    l96_two= L96TwoLevel_updated(save_dt=save_time_step, X_init=initial_X, Y_init=initial_Y,K=k, J=J, h=1, F=forcing, c=10, b=10, dt=time_step)\n",
    "    l96_two.iterate(mtu)\n",
    "    h2_xarray = l96_two.history\n",
    "    \n",
    "    x = np.ravel(h2_xarray.X)\n",
    "    x_subset = x.reshape(-1,k) #shape (timesteps, k)\n",
    "    advection = np.roll(x_subset, 1,axis=1) * (np.roll(x_subset, 2,axis=1) - np.roll(x_subset, -1,axis=1))\n",
    "    data = np.stack([x_subset,advection],axis=2)\n",
    "    y  = np.ravel(h2_xarray.Y).reshape(-1,k,J)\n",
    "    data_with_y = np.concatenate([data,y],axis=2)\n",
    "    \n",
    "    del h2_xarray\n",
    "    \n",
    "    np.save(data_file_path_to_save,data[1:])\n",
    "    np.save(data_with_y_file_path_to_save,data_with_y[1:])\n",
    "    \n",
    "    initX_new = x_subset[-1,:]\n",
    "    initY_new = y[-1,:,:].reshape(k*J)\n",
    "    \n",
    "    return initX_new, initY_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "J = 32\n",
    "burn_in_mtu = 2\n",
    "time_step = 0.001\n",
    "save_time_step = 0.005\n",
    "\n",
    "initX = np.zeros(shape=k)\n",
    "initY = np.zeros(shape=J*k)\n",
    "initX[0] = 1\n",
    "initY[0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training sets #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000 MTU for F = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = create_datasets_no_saving_of_xarray_burn_in(initX,initY,2000,\"../data/truth_run/training_dataset.npy\",\"../data/truth_run/training_dataset_with_y.npy\",20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 500 MTU for F = 21\n",
    "* 500 MTU for F = 19\n",
    "* 500 MTU for F = 20.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,_ = create_datasets_no_saving_of_xarray_burn_in(initX,initY,500,\n",
    "            \"../data/truth_run/climate_change_exp/train_f_21.npy\",\"../data/truth_run/climate_change_exp/train_f_21_with_y.npy\",\n",
    "                                   forcing=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,_=create_datasets_no_saving_of_xarray_burn_in(initX,initY,500,\n",
    "            \"../data/truth_run/climate_change_exp/train_f_19.npy\",\"../data/truth_run/climate_change_exp/train_f_19_with_y.npy\",\n",
    "                                   forcing=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,_=create_datasets_no_saving_of_xarray_burn_in(initX,initY,500,\n",
    "            \"../data/truth_run/climate_change_exp/train_f_20_5.npy\",\"../data/truth_run/climate_change_exp/train_f_20_5_with_y.npy\",\n",
    "                                   forcing=20.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_1 = np.load(\"../data/truth_run/climate_change_exp/train_f_21.npy\")\n",
    "\n",
    "training_2 = np.load(\"../data/truth_run/climate_change_exp/train_f_19.npy\")\n",
    "\n",
    "training_3 = np.load(\"../data/truth_run/climate_change_exp/train_f_20_5.npy\")\n",
    "\n",
    "ones_array = np.ones((training_1.shape[0],8,1))\n",
    "\n",
    "training_1 = np.concatenate([training_1,21*ones_array],axis=2)\n",
    "training_2 = np.concatenate([training_2,19*ones_array],axis=2)\n",
    "training_3 = np.concatenate([training_3,20.5*ones_array],axis=2)\n",
    "\n",
    "np.save(\"../data/truth_run/climate_change_exp/train_set_1.npy\",training_1)\n",
    "np.save(\"../data/truth_run/climate_change_exp/train_set_2.npy\",training_2)\n",
    "np.save(\"../data/truth_run/climate_change_exp/train_set_3.npy\",training_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500 MTU for F = 21.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,_=create_datasets_no_saving_of_xarray_burn_in(initX,initY,500,\n",
    "            \"../data/truth_run/climate_change_exp/val.npy\",\"../data/truth_run/climate_change_exp/val_with_y.npy\",\n",
    "                                   forcing=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell adds on a column with the value of F\n",
    "\n",
    "validation_set = np.load(\"../data/truth_run/climate_change_exp/val_21_5.npy\")\n",
    "\n",
    "ones_array = np.ones((validation_set.shape[0],8,1))\n",
    "\n",
    "validation_set = np.concatenate([validation_set,21.5*ones_array],axis=2)\n",
    "\n",
    "np.save(\"../data/truth_run/climate_change_exp/val_21_5.npy\",validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval sets #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the below for however many MTU you want. May need to break up into smaller chunks if 25000 MTU is too much to do in one go. Here we need at least 50000 MTU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F = 20 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data = np.load(\"../data/truth_run/training_dataset_with_y.npy\")\n",
    "\n",
    "initX_extra = initial_data[-1,:,0]\n",
    "initY_extra = initial_data[-1,:,2:].reshape(k*J)\n",
    "\n",
    "_, _ = create_datasets_no_saving_of_xarray(initX_extra,initY_extra,25000,\"../data/truth_run/extra.npy\",\"../data/truth_run/extra_with_y.npy\",20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = np.load(\"../data/truth_run/extra_with_y.npy\")\n",
    "\n",
    "initX_extra2 = extra[-1,:,0]\n",
    "initY_extra2 = extra[-1,:,2:].reshape(k*J)\n",
    "\n",
    "_, _ = create_datasets_no_saving_of_xarray(initX_extra2,initY_extra2,25000,\"../data/truth_run/extra2.npy\",\"../data/truth_run/extra2_with_y.npy\",20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F = 28 ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,_ = create_datasets_no_saving_of_xarray_burn_in(initX,initY,25000,\n",
    "            \"../data/truth_run/climate_change_exp/test_1.npy\",\"../data/truth_run/climate_change_exp/test_1_y.npy\",\n",
    "                                   forcing=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = np.load(\"../data/truth_run/climate_change_exp/test_1_y.npy\")\n",
    "\n",
    "initX_extra = set1[-1,:,0]\n",
    "initY_extra = set1[-1,:,2:].reshape(k*J)\n",
    "\n",
    "_,_ = create_datasets_no_saving_of_xarray(initX_extra,initY_extra,25000,\n",
    "            \"../data/truth_run/climate_change_exp/test_2.npy\",\"../data/truth_run/climate_change_exp/test_2_y.npy\",\n",
    "                                   forcing=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this procedure, truth data also needs to be generated for F = 21.5, 32, 35 and 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
