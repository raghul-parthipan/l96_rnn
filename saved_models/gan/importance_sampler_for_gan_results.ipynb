{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run section 1 and 2 to prepare data and load model.\n",
    "\n",
    "Section 3 does the loglik estimation and the confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be OOM issues when dealing with the long test files. If so, then the loglik will need to be computed in batches (not shown here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "from pickle import load\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "K = keras.backend\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,PowerTransformer\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from helper import *\n",
    "K.set_floatx(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below if using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using laptop gpu\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "J = 32\n",
    "save_time_step = 0.005\n",
    "h=1 \n",
    "c=10\n",
    "b=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_length = 100\n",
    "test_seq_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train, validation and test datasets \n",
    "train_dataset = np.load(\"../../data/truth_run/training_dataset.npy\")\n",
    "valid_dataset = np.load(\"../../data/truth_run/val_dataset.npy\")\n",
    "test_dataset   = np.load(\"../../data/truth_run/climate_eval_dataset.npy\")\n",
    "test_dataset_23 = np.load(\"../../data/truth_run/climate_change_exp/full_test_set.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_dataset[:,:,0]\n",
    "x_valid = valid_dataset[:,:,0]\n",
    "x_test = test_dataset[:,:,0]\n",
    "x_test_23 = test_dataset_23[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Functions to work out the exact U for each x #########\n",
    "\n",
    "def _rhs_X_dt(X, F,U,dt=0.005):\n",
    "    \"\"\"Compute the right hand side of the X-ODE.\"\"\"\n",
    "\n",
    "    dXdt = (-np.roll(X, 1,axis=1) * (np.roll(X, 2,axis=1) - np.roll(X, -1,axis=1)) -\n",
    "                X + F - U)\n",
    "\n",
    "    return dt * dXdt \n",
    "\n",
    "\n",
    "def U(Xt,Xt_1,F,dt=0.005):\n",
    "    k1_X = _rhs_X_dt(Xt, F,U=0)\n",
    "    k2_X = _rhs_X_dt(Xt + k1_X / 2, F,U=0)\n",
    "    Xt_1_pred = k2_X + Xt \n",
    "    #print(Xt_1_pred)\n",
    "    Ut = (Xt_1_pred - Xt_1 )/dt\n",
    "\n",
    "    return Ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_t = U(x_train[:-1,:],x_train[1:,:],20)    \n",
    "u_t_valid = U(x_valid[:-1,:],x_valid[1:,:],20)  \n",
    "u_t_test = U(x_test[:-1,:],x_test[1:,:],20)\n",
    "u_t_test_23 = U(x_test_23[:-1,:],x_test_23[1:,:],23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.stack([x_train[:-1],u_t],axis=2)\n",
    "valid_dataset = np.stack([x_valid[:-1],u_t_valid],axis=2)\n",
    "test_dataset =  np.stack([x_test[:-1],u_t_test],axis=2)\n",
    "test_dataset_23 =  np.stack([x_test_23[:-1],u_t_test_23],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets_for_RNN(dataset,history_length):\n",
    "    max_index = (dataset.shape[0]-1)//history_length\n",
    "    dataset = dataset[:(max_index*history_length +1),:,:] \n",
    "    dataset_shape = dataset.shape[0]\n",
    "    last_elements = dataset[-1,:,:]\n",
    "    remaining_dataset = dataset[:-1,:,:]\n",
    "    reshaped = remaining_dataset.reshape(-1,history_length,k,2)\n",
    "    add_on = reshaped[1:,:1,:,:]\n",
    "    last_elements = last_elements.reshape(1,1,k,2)\n",
    "    add_on_combined = np.concatenate((add_on,last_elements),axis=0)\n",
    "    concat = np.concatenate((reshaped,add_on_combined),axis=1)\n",
    "    concat = concat.transpose((2,0,1,3)).reshape((-1,history_length+1,2),order=\"F\")\n",
    "    return concat.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_for_loglik_estimation(dataset,history_length,exp_its,conf_its):\n",
    "    dataset = dataset.astype(\"float32\")\n",
    "    max_index = (dataset.shape[0]-1)//history_length\n",
    "    dataset = dataset[:(max_index*history_length +1),:,:] \n",
    "    dataset_shape = dataset.shape[0]\n",
    "    last_elements = dataset[-1,:,:]\n",
    "    remaining_dataset = dataset[:-1,:,:]\n",
    "    reshaped = remaining_dataset.reshape(-1,history_length,k,2)\n",
    "    add_on = reshaped[1:,:1,:,:]\n",
    "    last_elements = last_elements.reshape(1,1,k,2)\n",
    "    add_on_combined = np.concatenate((add_on,last_elements),axis=0)\n",
    "    concat = np.concatenate((reshaped,add_on_combined),axis=1)\n",
    "    new = np.repeat(concat.transpose((0,2,1,3)),exp_its,axis=0).reshape(-1,history_length+1,2)\n",
    "    new = np.tile(new,(conf_its,1,1))\n",
    "    return new.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn_features = prepare_datasets_for_RNN(training_dataset,history_length)\n",
    "valid_nn_features = prepare_datasets_for_RNN(valid_dataset,test_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_loglik = prep_for_loglik_estimation(test_dataset[:],6000,1,1)\n",
    "test_for_loglik_23 = prep_for_loglik_estimation(test_dataset_23[:],6000,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264, 6001, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_for_loglik_23.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(train_nn_features[:,:,0])\n",
    "x_std = np.std(train_nn_features[:,:,0])\n",
    "u_mean = np.mean(train_nn_features[:,:,1])\n",
    "u_std = np.std(train_nn_features[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "train_nn_features[:,:,0] = (train_nn_features[:,:,0] - x_mean)/x_std\n",
    "train_nn_features[:,:,1] = (train_nn_features[:,:,1] - u_mean)/u_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "valid_nn_features[:,:,0] = (valid_nn_features[:,:,0] - x_mean)/x_std\n",
    "valid_nn_features[:,:,1] = (valid_nn_features[:,:,1] - u_mean)/u_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "test_for_loglik[:,:,0] = (test_for_loglik[:,:,0] - x_mean)/x_std\n",
    "test_for_loglik[:,:,1] = (test_for_loglik[:,:,1] - u_mean)/u_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "test_for_loglik_23[:,:,0] = (test_for_loglik_23[:,:,0] - x_mean)/x_std\n",
    "test_for_loglik_23[:,:,1] = (test_for_loglik_23[:,:,1] - u_mean)/u_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_loglik_23_tf = tf.convert_to_tensor(test_for_loglik_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_for_loglik_tf = tf.convert_to_tensor(test_for_loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_shape = 34\n",
    "hidden_state_size_bi = 32\n",
    "encoder_hidden_state_size=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "generator = keras.models.load_model(\"gan_generator_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "h_encoder = keras.models.load_model(\"h_encoder_final.h5\",custom_objects={\n",
    "           \"Sampling\":Sampling})\n",
    "bi_rnn = keras.models.load_model(\"bi_rnn_final.h5\")\n",
    "h_encoder_first = keras.models.load_model(\"h_encoder_first_final.h5\",custom_objects={\n",
    "           \"Sampling\":Sampling})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_encoder(xu_seq,encoder,first_encoder,encoder_hidden_state_size,bi_rnn):\n",
    "    \n",
    "    length = xu_seq.shape[1]\n",
    "    batch_shape = xu_seq.shape[0]\n",
    "    \n",
    "    h_sequence = tf.TensorArray(dtype=tf.float32,size=0,dynamic_size=True)\n",
    "    h_mean_out = tf.TensorArray(dtype=tf.float32,size=0,dynamic_size=True)\n",
    "    h_log_var_out = tf.TensorArray(dtype=tf.float32,size=0,dynamic_size=True)\n",
    "    \n",
    "    u_summary = bi_rnn(xu_seq[:,:-1,:])\n",
    "\n",
    "    h_mean1,h_log_var1,h_prev = first_encoder.predict([xu_seq[:,0,0],u_summary[:,0,0]])\n",
    "    h_sequence = h_sequence.write(0,h_prev)\n",
    "    h_mean_out = h_mean_out.write(0,h_mean1)\n",
    "    h_log_var_out = h_log_var_out.write(0,h_log_var1)\n",
    "\n",
    "    \n",
    "    hidden_state_1 = tf.zeros(shape=(batch_shape,encoder_hidden_state_size))\n",
    "    hidden_state_2 = tf.zeros(shape=(batch_shape,encoder_hidden_state_size))    \n",
    "    \n",
    "    for n in tf.range(0,length-2):\n",
    "        h_mean,h_log_var,h_sample,state,state2 = encoder.predict([h_prev,u_summary[:,n+1:n+2,:],xu_seq[:,n+1:n+2,:1],\n",
    "                                                    hidden_state_1,hidden_state_2])\n",
    "        \n",
    "        h_sequence = h_sequence.write(n+1,h_sample)\n",
    "        h_prev = h_sample\n",
    "        h_mean_out = h_mean_out.write(n+1,h_mean)\n",
    "        h_log_var_out = h_log_var_out.write(n+1,h_log_var) \n",
    "        hidden_state_1 = state  \n",
    "        hidden_state_2 = state2     \n",
    "        \n",
    "    h_sequence = h_sequence.stack()        \n",
    "    h_mean_out_enc = h_mean_out.stack()\n",
    "    h_log_var_out = h_log_var_out.stack()\n",
    "    h_sequence = tf.transpose(h_sequence[:,:,0,:],[1,0,2])            \n",
    "    h_mean_out_enc = tf.transpose(h_mean_out_enc[:,:,0,:],[1,0,2])\n",
    "    h_log_var_out = tf.transpose(h_log_var_out[:,:,0,:],[1,0,2])\n",
    "\n",
    "    return h_sequence,h_mean_out_enc,h_log_var_out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loglik estimations #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loglik_gaussian_u_cond_h(xu_seq,h_encoding,sigma):\n",
    "    x_array =  xu_seq[:,:-1,:1]\n",
    "    x_array_reshape = tf.reshape(x_array,(-1,1))\n",
    "    u_array_reshape = tf.reshape(xu_seq[:,:-1,1:2],(-1,1))\n",
    "    h_encoding_reshape = tf.reshape(h_encoding,(-1,z_shape))\n",
    "    mean_u = generator([x_array_reshape,h_encoding_reshape])\n",
    "    term = -K.log((sigma**2) *2*math.pi) - tf.math.divide((u_array_reshape-mean_u),sigma)**2\n",
    "    loglik = 0.5*term\n",
    "    loglik = tf.reshape(loglik,(-1,xu_seq.shape[1]-1,1))\n",
    "    return tf.reduce_sum(loglik,axis=1) #take sum over time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loglik_gaussian_h_encoder(h_encoding,h_mean,h_logvar):\n",
    "    term1 = -(1/2)*(K.log(2*math.pi) + h_logvar)\n",
    "    term2 = -((h_encoding-h_mean)**2)/(2*K.exp(h_logvar))\n",
    "    loglik = term1+term2\n",
    "    loglik = tf.reduce_sum(loglik,axis=[2],keepdims=True) #sum over the h dimensions \n",
    "    return tf.reduce_sum(loglik,axis=1) #sum over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loglik_gaussian_h_gan(h_encoding):\n",
    "    #### term 1 ####\n",
    "    ### h drawn from normal(0,1) ####\n",
    "    term = tf.reduce_sum(0.5*(-K.log((1**2) *2*math.pi) - tf.math.divide((h_encoding[:,:1,:]),1)**2),axis=2) #sum over z dimensions\n",
    "\n",
    "    #### term 2 #####\n",
    "    #### loglik for the rest of the markovian seq ####\n",
    "    array = h_encoding[:,1:,:]\n",
    "    phi = 0.7486\n",
    "    mean = h_encoding[:,:-1,:]*phi\n",
    "    sigma = (1-phi**2)**0.5\n",
    "    \n",
    "    term2 = 0.5*(-K.log((sigma**2) *2*math.pi) - tf.math.divide((array-mean),sigma)**2)\n",
    "    term2 = tf.reduce_sum(term2,axis=[2]) #sum over the h dimensions \n",
    "    \n",
    "    loglik_array = tf.concat([term,term2 ],axis=1)\n",
    "    loglik = tf.reduce_sum(loglik_array,axis=1,keepdims=True) #sum over t\n",
    "                          \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglik_exact_approx(testing_sequence,sigma,it):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    it is number for approx\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    combined = tf.zeros(shape=(int(testing_sequence.shape[0]/k),1))\n",
    "    \n",
    "    print(\"expectation_outer_loop\")\n",
    "    \n",
    "    for n in tf.range(it):\n",
    "        \n",
    "        print(n)\n",
    "    \n",
    "        h_sequence,h_mean_out_enc,z_log_var_out = sample_from_encoder(testing_sequence,h_encoder,h_encoder_first,\n",
    "                                                    encoder_hidden_state_size,bi_rnn)\n",
    "\n",
    "        print(\"generated\")\n",
    "        exponent = eval_loglik_gaussian_u_cond_h(testing_sequence,h_sequence,sigma) +  eval_loglik_gaussian_h_gan(h_sequence) \\\n",
    "          - eval_loglik_gaussian_h_encoder(h_sequence,h_mean_out_enc,z_log_var_out)\n",
    "\n",
    "        reshape_exponent = tf.reshape(exponent,(-1,k))\n",
    "        exponent_summed = tf.reduce_sum(reshape_exponent,axis=1)\n",
    "        \n",
    "        reshaped = tf.reshape(exponent_summed,(-1,1)) \n",
    "        \n",
    "        combined = tf.concat([combined,reshaped],axis=1)\n",
    "        \n",
    "    result = combined[:,1:]\n",
    "    \n",
    "    most_positive_alpha = np.max(result,axis=1)\n",
    "    remaining_factor = result[:,:] - most_positive_alpha.reshape(-1,1)[:,:]\n",
    "    second_positive = np.sort(remaining_factor,axis=1)[:,-2]\n",
    "\n",
    "    x = np.exp(second_positive.astype(\"float64\"))\n",
    "    term3 = x - (x**2)/2    \n",
    "    \n",
    "    loglik = -np.log(float(result.shape[1])) + most_positive_alpha +term3\n",
    "    \n",
    "    avg_loglik = np.sum(loglik) / ((testing_sequence.shape[0])*(testing_sequence.shape[1]-1))\n",
    "    \n",
    "    print(avg_loglik)\n",
    "    \n",
    "    return avg_loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval_arrays(num_its_conf,testing_sequence,sigma,it_mc):\n",
    "    \n",
    "    #testing_sequence must have testing_sequence.shape[0] be a multiple of K\n",
    "    \n",
    "    array = np.zeros(shape=(num_its_conf,1))\n",
    "    \n",
    "    for i in tf.range(num_its_conf):\n",
    "        \n",
    "        print(\"confidence iteration\",i)\n",
    "        \n",
    "        loglik = loglik_exact_approx(testing_sequence,sigma,it_mc)\n",
    "        array[i,0] = loglik\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rx_star(array):\n",
    "    return np.random.choice(array,size=len(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F = 20 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = confidence_interval_arrays(50,test_for_loglik_tf,0.001,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"loglik_20.npy\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"loglik_20.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.005\n",
    "\n",
    "a_with_correction = a -np.log(dt*u_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the correction is explained in the Appendix of the paper. We need to add a scaling factor to get loglik of x sequence. And just like for the RNN, we also include u_std here as we scale the variables when training the models (and just as how the factor dt comes from a change of variables for likelihood, so too does the u_std factor as we change from the variable U_scaled back to U_unscaled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sample = [np.mean(rx_star(a_with_correction.reshape(-1))) for _ in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-37.693616463069645"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a_with_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-37.69651479547297\n",
      "-37.690796590341705\n"
     ]
    }
   ],
   "source": [
    "lo, hi = np.quantile(a_sample,[0.025,0.975])\n",
    "print(lo)\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F = 23 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = confidence_interval_arrays(50,test_for_loglik_23_tf,0.001,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"loglik_23.npy\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(\"loglik_23.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.005\n",
    "\n",
    "a_with_correction = a -np.log(dt*u_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sample = [np.mean(rx_star(a_with_correction.reshape(-1))) for _ in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-44.859477832697436"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a_with_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-44.86401038957839\n",
      "-44.85502655197918\n"
     ]
    }
   ],
   "source": [
    "lo, hi = np.quantile(a_sample,[0.025,0.975])\n",
    "print(lo)\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
